@book{marohn2019strong,
  title={Strong towns: A bottom-up revolution to rebuild American prosperity},
  author={Marohn Jr, Charles L},
  year={2019},
  publisher={John Wiley \& Sons}
}

@article{westlake2019happy,
  title={Happy city: transforming our lives through urban design},
  author={Westlake, Gregory M},
  journal={Existential Analysis},
  volume={30},
  number={2},
  pages={388--392},
  year={2019},
  publisher={Society for Existential Analysis}
}

@article{marshall1996injuries,
  title={Injuries related to car crime: the joy-riding epidemic},
  author={Marshall, C and Boyd, KT and Moran, CG},
  journal={Injury},
  volume={27},
  number={2},
  pages={79--80},
  year={1996},
  publisher={Elsevier}
}

@article{norton2007street,
  title={Street rivals: Jaywalking and the invention of the motor age street},
  author={Norton, Peter D},
  journal={Technology and culture},
  volume={48},
  number={2},
  pages={331--359},
  year={2007},
  publisher={JSTOR}
}

@book{norton2011fighting,
  title={Fighting traffic: the dawn of the motor age in the American city},
  author={Norton, Peter D},
  year={2011},
  publisher={Mit Press}
}

@incollection{pucher1996united,
  title={The United States: the car-dependent society},
  author={Pucher, John and Lefevre, Christian},
  booktitle={The urban transport crisis in Europe and North America},
  pages={175--200},
  year={1996},
  publisher={Springer}
}

@book{parolek2020missing,
  title={Missing Middle Housing: Thinking Big and Building Small to Respond to Today's Housing Crisis},
  author={Parolek, Daniel G},
  year={2020},
  publisher={Island Press}
}

@book{decicco2006global,
  title={Global warming on the road: the climate impact of America's automobiles},
  author={DeCicco, John and Fung, Freda and An, Feng},
  year={2006},
  publisher={Environmental Defense}
}

@article{gately2015cities,
  title={Cities, traffic, and CO2: A multidecadal assessment of trends, drivers, and scaling relationships},
  author={Gately, Conor K and Hutyra, Lucy R and Sue Wing, Ian},
  journal={Proceedings of the National Academy of Sciences},
  volume={112},
  number={16},
  pages={4999--5004},
  year={2015},
  publisher={National Acad Sciences}
}

@article{milton1998relationship,
  title={The Relationship Among Highway Geometries},
  author={Milton, John and Mannering, Fred},
  year={1998}
}

@article{abdel2000modeling,
  title={Modeling traffic accident occurrence and involvement},
  author={Abdel-Aty, Mohamed A and Radwan, A Essam},
  journal={Accident Analysis \& Prevention},
  volume={32},
  number={5},
  pages={633--642},
  year={2000},
  publisher={Elsevier}
}

@misc{jette2013book,
  title={Book Review: Happy City: Transforming Our Lives through Urban Design, by Charles Montgomery},
  author={Jette, Susan},
  year={2013},
  publisher={SAGE Publications Sage CA: Los Angeles, CA}
}

@book{marohn2021confessions,
  title={Confessions of a Recovering Engineer: Transportation for a Strong Town},
  author={Marohn Jr, Charles L},
  year={2021},
  publisher={John Wiley \& Sons}
}

@book{marohn2012thoughts,
  title={Thoughts on building strong towns},
  author={Marohn, Charles L},
  year={2012},
  publisher={CreateSpace Independent Publishing Platform}
}

@article{motorvehicle2020,
title={National Center for Statistics and Analysis. (2020, December). Early estimate of motor vehicle traffic fatalities for the first 9 months (jan-sep) of 2020 (crash stats brief statistical summary. report No. DOT HS 813 053).}, 
author={National Highway Traffic Safety Administration},
year={2020}
}

@article{bates2015mixed,
  title={Mixed-effects models using lme4},
  author={Bates, Douglas and Maechler, Martin and Bolker, Ben and Walker, Steven},
  journal={J Stat Softw},
  volume={67},
  number={1},
  pages={1--48},
  year={2015}
}

@inproceedings{hoult2009wireless,
  title={Wireless sensor networks: creating smart infrastructure},
  author={Hoult, Neil and Bennett, Peter J and Stoianov, Ivan and Fidler, Paul and Maksimovi edo and Middleton, Campbell and Graham, Nigel and Soga, Kenichi},
  booktitle={Proceedings of the Institution of Civil Engineers-Civil Engineering},
  volume={162},
  number={3},
  pages={136--143},
  year={2009},
  organization={Thomas Telford Ltd}
}

@inproceedings{salo2021ai,
  title={AI Ethics-Critical Reflections on Embedding Ethical Frameworks in AI Technology},
  author={Salo-Pontinen, Henrikki},
  booktitle={International Conference on Human-Computer Interaction},
  pages={311--329},
  year={2021},
  organization={Springer}
}

@article{sukhadia2020optimization,
  title={Optimization of smart traffic governance system using artificial intelligence},
  author={Sukhadia, Aayush and Upadhyay, Khush and Gundeti, Meghashree and Shah, Smit and Shah, Manan},
  journal={Augmented Human Research},
  volume={5},
  number={1},
  pages={1--14},
  year={2020},
  publisher={Springer}
}

@article{wei2021recent,
  title={Recent advances in reinforcement learning for traffic signal control: A survey of models and evaluation},
  author={Wei, Hua and Zheng, Guanjie and Gayah, Vikash and Li, Zhenhui},
  journal={ACM SIGKDD Explorations Newsletter},
  volume={22},
  number={2},
  pages={12--18},
  year={2021},
  publisher={ACM New York, NY, USA}
}

@article{treiber2000congested,
  title={Congested traffic states in empirical observations and microscopic simulations},
  author={Treiber, Martin and Hennecke, Ansgar and Helbing, Dirk},
  journal={Physical review E},
  volume={62},
  number={2},
  pages={1805},
  year={2000},
  publisher={APS}
}

@book{Sutton2018,
  abstract = {This chapter provides a concise introduction to Reinforcement Learning (RL) from a machine learning perspective. It provides the required background to understand the chapters related to RL in this book. It makes no assumption on previous knowledge in this research area and includes short descriptions of some of the latest trends, which are normally excluded from other introductions or overviews on RL. The chapter provides more emphasis on the general conceptual framework and ideas of RL rather than on presenting a rigorous mathematical discussion that may require a great deal of effort by the reader. The first section provides a general introduction to the area. The following section describes the most common solution techniques. In the third section, some of the most recent techniques proposed to deal with large search spaces are described. Finally, the last section provides some final remarks and current research challenges in RL. {\textcopyright} 2012, IGI Global.},
  author = {Sutton, Richard S. and Barto, Andrew G.},
  file = {:C\:/Users/Daniel Kuknyo/Documents/ELTE/2/Artificial Intelligence in Processes and Automation/Project/Report/References/book - Reinforcement Learning An Introduction, 2nd Edition by Richard S. Sutton, Andrew G Barto (z-lib.org).pdf:pdf},
  isbn = {9780262039246},
  pages = {481},
  title = {{Reinforcement Leaning}},
  year = {2018}
}
@article{Sutton2017,
  abstract = {Reinforcement learning, one of the most active research areas in artificial intelligence, is a computational approach to learning whereby an agent tries to maximize the total amount of reward it receives when interacting with a complex, uncertain environment. In Reinforcement Learning, Richard Sutton and Andrew Barto provide a clear and simple account of the key ideas and algorithms of reinforcement learning. Their discussion ranges from the history of the field's intellectual foundations to the most recent developments and applications. The only necessary mathematical background is familiarity with elementary concepts of probability.The book is divided into three parts. Part I defines the reinforcement learning problem in terms of Markov decision processes. Part II provides basic solution methods: dynamic programming, Monte Carlo methods, and temporal-difference learning. Part III presents a unified view of the solution methods and incorporates artificial neural networks, eligibility traces, and planning; the two final chapters present case studies and consider the future of reinforcement learning.},
  author = {Sutton, Richard S. and Barto, Andrew G.},
  doi = {10.1016/s1364-6613(99)01331-5},
  file = {:C\:/Users/Daniel Kuknyo/Documents/ELTE/2/Artificial Intelligence in Processes and Automation/Project/Report/References/bookdraft2017nov5.pdf:pdf},
  issn = {13646613},
  journal = {Trends in Cognitive Sciences},
  number = {9},
  pages = {360},
  title = {{Reinforcement Learning: An Introduction, draft 2017 nov.5}},
  volume = {3},
  year = {2017}
}
@misc{Lanham2020,
  abstract = {The AI revolution is here and it is embracing games. Game developers are being challenged to enlist cutting edge AI as part of their games. In this book, you will look at the journey of building capable AI using reinforcement learning algorithms and techniques. You will learn to solve complex tasks and build next-generation games using a ...},
  author = {Lanham, Micheal},
  file = {:C\:/Users/Daniel Kuknyo/Documents/ELTE/2/Artificial Intelligence in Processes and Automation/Project/Report/References/Hands-On Reinforcement Learning for Games Implementing self-learning agents in games using artificial intelligence techniques (Micheal Lanham) (z-lib.org).pdf:pdf},
  isbn = {9781839214936},
  title = {{Hands-on reinforcement learning for games : implementing self-learning agents in games using artificial intelligence techniques}},
  year = {2020}
}
@article{Yu2019,
  abstract = {The main objective of reinforcement learning (RL) is to enable an agent to act optimally to maximize the cumulative long-term reward. Q-learning is a model free RL algorithm, which iteratively learns a long-term reward function "Q" given the current state and action. The Deep Q-Learning Network (DQN) facilitates the Q-learning by modeling the Q-function as a neural network. This project implements and experiments such DQN models on the OpenAI Gym's LunarLander-v2 environment, using a two-layer feed-forward network with a technique named "experience replay". Extensive experiments are done to determine the neural network size and tune various hyper-parameters including learning rate \alpha, reward discount factor \gamma and exploration-exploitation trade-off \epsilon. Major findings include 1) the Lunar Lander favors a large hidden layer but not a deeper network; 2) a near-one reward discount is necessary for the model to consider final successful landing. Finally, our best model can stably achieve 280+ mean reward for a trial of 100 landing episodes. The code can be found at https://github.com/XinliYu/RL-Projects.},
  author = {Yu, Xinli},
  file = {:C\:/Users/Daniel Kuknyo/Documents/ELTE/2/Artificial Intelligence in Processes and Automation/Project/Report/References/dqn_lunar_lander.pdf:pdf},
  number = {May},
  pages = {5},
  title = {{Deep Q-Learning on Lunar Lander Game}},
  url = {https://www.researchgate.net/publication/333145451_Deep_Q-Learning_on_Lunar_Lander_Game},
  year = {2019}
}
@article{xu2018deep,
  title={Deep reinforcement learning with sarsa and Q-learning: a hybrid approach},
  author={Xu, Zhi-xiong and Cao, Lei and Chen, Xi-liang and Li, Chen-xi and Zhang, Yong-liang and Lai, Jun},
  journal={IEICE TRANSACTIONS on Information and Systems},
  volume={101},
  number={9},
  pages={2315--2322},
  year={2018},
  publisher={The Institute of Electronics, Information and Communication Engineers}
}
@article{mnih2015human,
  title={Human-level control through deep reinforcement learning},
  author={Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A and Veness, Joel and Bellemare, Marc G and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K and Ostrovski, Georg and others},
  journal={nature},
  volume={518},
  number={7540},
  pages={529--533},
  year={2015},
  publisher={Nature Publishing Group}
}
@article{lin1992self,
  title={Self-improving reactive agents based on reinforcement learning, planning and teaching},
  author={Lin, Long-Ji},
  journal={Machine learning},
  volume={8},
  number={3},
  pages={293--321},
  year={1992},
  publisher={Springer}
}
@article{hasselt2010double,
  title={Double Q-learning},
  author={Hasselt, Hado},
  journal={Advances in neural information processing systems},
  volume={23},
  year={2010}
}
@article{simonini2018improvements,
  title={Improvements in Deep Q Learning: Dueling Double DQN, Prioritized Experience Replay, and fixed Q-targets},
  author={Simonini, Thomas},
  year={2018},
  publisher={{\v{C}}ervenec}
}
@inproceedings{wang2016dueling,
  title={Dueling network architectures for deep reinforcement learning},
  author={Wang, Ziyu and Schaul, Tom and Hessel, Matteo and Hasselt, Hado and Lanctot, Marc and Freitas, Nando},
  booktitle={International conference on machine learning},
  pages={1995--2003},
  year={2016},
  organization={PMLR}
}
